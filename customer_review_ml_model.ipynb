{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mayeem-research/customer_review_ml_model./blob/main/customer_review_ml_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O68tUWfinJXs"
      },
      "source": [
        "# Importing necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "dD7-LOiCnUfc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, GlobalMaxPooling1D, Dense\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import numpy as np  # ‚úÖ Import NumPy\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.layers import Embedding, GlobalMaxPooling1D, Dense\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mdaRVCznenT"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "collapsed": true,
        "id": "qCv4Y5OinfTY",
        "outputId": "fc75e98e-2140-479f-80c4-56868f9c02e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset Sample:\n",
            "                                           T75of src                reviewer  \\\n",
            "0  https://play-lh.googleusercontent.com/a-/ALV-U...           Manuel Mintah   \n",
            "1  https://play-lh.googleusercontent.com/a/ACg8oc...   Eliasito Markmilliano   \n",
            "2  https://play-lh.googleusercontent.com/a/ACg8oc...  Issahaku Mohammed-Awal   \n",
            "3  https://play-lh.googleusercontent.com/a-/ALV-U...          Andy O. Appiah   \n",
            "4  https://play-lh.googleusercontent.com/a-/ALV-U...   Michael Anyetei Adjei   \n",
            "\n",
            "                 date                                             review  \\\n",
            "0   December 24, 2023  I have personally experienced the power of thi...   \n",
            "1  September 22, 2023  This new GCB app has left quite an impression ...   \n",
            "2    November 5, 2023  I think you deserve a 5 star. At first, I coul...   \n",
            "3   December 14, 2023  I have installed this app many time in differe...   \n",
            "4  September 18, 2023  This has to be one of the best banking apps ou...   \n",
            "\n",
            "                                 AJTPZc        I6j64d              I9Jtec  \\\n",
            "0  186 people found this review helpful  GCB Bank PLC    January 11, 2024   \n",
            "1  477 people found this review helpful  GCB Bank PLC  September 20, 2023   \n",
            "2  112 people found this review helpful  GCB Bank PLC  September 22, 2023   \n",
            "3   77 people found this review helpful  GCB Bank PLC   December 15, 2023   \n",
            "4   50 people found this review helpful  GCB Bank PLC  September 20, 2023   \n",
            "\n",
            "                                              ras4vb  \n",
            "0  Hi Manuel,\\nThanks for your positive feedback ...  \n",
            "1  Feedback is well appreciatedüëç. Keep on enjoyin...  \n",
            "2  We appreciate your feedback. We're sorry to he...  \n",
            "3  Hi Andy,\\nWe regret the challenges you've enco...  \n",
            "4  Hi Michael,\\nWe appreciate your feedback. We a...  \n",
            "The 'sentiment' column is missing. Creating a sample 'sentiment' column.\n",
            "Data split successfully.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "file_url = '/content/drive/MyDrive/Colab Notebooks/play_review.csv'\n",
        "\n",
        "try:\n",
        "    # Load the dataset\n",
        "    df = pd.read_csv(file_url, on_bad_lines='skip')\n",
        "    print(\"Dataset Sample:\")\n",
        "    print(df.head())\n",
        "\n",
        "    # Check if 'review' column exists\n",
        "    if 'review' not in df.columns:\n",
        "        raise KeyError(\"The dataset must contain a 'review' column for text data.\")\n",
        "\n",
        "    # Add a simple 'sentiment' column if it doesn't exist\n",
        "    if 'sentiment' not in df.columns:\n",
        "        print(\"The 'sentiment' column is missing. Creating a sample 'sentiment' column.\")\n",
        "        df['sentiment'] = df['review'].apply(lambda x: 'positive' if 'good' in str(x).lower() or 'love' in str(x).lower() else 'negative')\n",
        "\n",
        "    # Split the dataset into training and testing sets\n",
        "    train_reviews, test_reviews, train_labels, test_labels = train_test_split(\n",
        "        df['review'],\n",
        "        df['sentiment'],\n",
        "        test_size=0.2,  # 20% of the data will be used for testing\n",
        "        random_state=42  # Seed for reproducibility\n",
        "    )\n",
        "    print(\"Data split successfully.\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"File not found: {file_url}. Please check the file path.\")\n",
        "except KeyError as e:\n",
        "    print(f\"KeyError: {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXBA7x2Mre3b"
      },
      "source": [
        "######################### Pre-processing #########################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Avz-IxqurfM7",
        "outputId": "fc9b43b1-2b17-4be8-f291-f6eaf27ebb56"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_train samples: 336\n",
            "y_train samples: 336\n",
            "[1 1]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "\n",
        "# Example Training Data\n",
        "train_reviews = [\"Great product!\", \"Terrible service.\", \"Loved it!\", \"Worst experience ever.\"] * 84  # Now 336 samples\n",
        "y_train = [1, 0, 1, 0] * 84  # 336 labels (same as train_reviews)\n",
        "\n",
        "test_reviews = [\"Amazing!\", \"Horrible.\"]\n",
        "\n",
        "# Convert Text to TF-IDF Vectors\n",
        "vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
        "X_train = vectorizer.fit_transform(train_reviews)\n",
        "X_test = vectorizer.transform(test_reviews)\n",
        "\n",
        "# Ensure matching sizes\n",
        "print(f\"X_train samples: {X_train.shape[0]}\")\n",
        "print(f\"y_train samples: {len(y_train)}\")\n",
        "\n",
        "# Train Logistic Regression Model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make Predictions\n",
        "predictions = model.predict(X_test)\n",
        "print(predictions)  # Expected Output: [1 0] (Depends on training data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ws_wwgV9e0Xj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P03swyN8dI3W",
        "outputId": "da7cf0c5-256c-4d3c-a511-f9cf4fda7795"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21liFzNmrsOL"
      },
      "source": [
        "######################## Model Development #########################\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "pVIqgtwhrtQY",
        "outputId": "6609e1c0-f9ab-465b-8a9c-53a7c828322c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_train shape: (336, 200), y_train shape: (336,)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 61ms/step - accuracy: 0.9188 - loss: 0.6696 - val_accuracy: 0.7500 - val_loss: 0.6719\n",
            "Epoch 2/10\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 0.6059 - val_accuracy: 0.7500 - val_loss: 0.6564\n",
            "Epoch 3/10\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 0.5460 - val_accuracy: 0.7500 - val_loss: 0.6410\n",
            "Epoch 4/10\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 0.4839 - val_accuracy: 0.7500 - val_loss: 0.6253\n",
            "Epoch 5/10\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 0.4237 - val_accuracy: 0.7500 - val_loss: 0.6096\n",
            "Epoch 6/10\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 1.0000 - loss: 0.3641 - val_accuracy: 0.7500 - val_loss: 0.5940\n",
            "Epoch 7/10\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 0.3072 - val_accuracy: 0.7500 - val_loss: 0.5805\n",
            "Epoch 8/10\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 1.0000 - loss: 0.2564 - val_accuracy: 0.7500 - val_loss: 0.5685\n",
            "Epoch 9/10\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 0.2145 - val_accuracy: 0.7500 - val_loss: 0.5581\n",
            "Epoch 10/10\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.1775 - val_accuracy: 0.7500 - val_loss: 0.5499\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ‚úÖ Define train and test reviews with equal sample size\n",
        "train_reviews = [\"Great product!\", \"Terrible service.\", \"Loved it!\", \"Worst experience ever.\"] * 84\n",
        "test_reviews = [\"Amazing!\", \"Horrible.\", \"Loved it.\", \"Never again!\"] * 21\n",
        "\n",
        "# ‚úÖ Ensure labels match the review count\n",
        "y_train = np.array([1, 0, 1, 0] * 84)  # 336 labels\n",
        "y_test = np.array([1, 0, 1, 0] * 21)   # 84 labels\n",
        "\n",
        "# ‚úÖ Tokenization\n",
        "tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(train_reviews)\n",
        "\n",
        "X_train_seq = tokenizer.texts_to_sequences(train_reviews)\n",
        "X_test_seq = tokenizer.texts_to_sequences(test_reviews)\n",
        "\n",
        "# ‚úÖ Pad sequences\n",
        "X_train_padded = pad_sequences(X_train_seq, maxlen=200, padding='post', truncating='post')\n",
        "X_test_padded = pad_sequences(X_test_seq, maxlen=200, padding='post', truncating='post')\n",
        "\n",
        "# ‚úÖ Check data shape\n",
        "print(f\"X_train shape: {X_train_padded.shape}, y_train shape: {y_train.shape}\")  # Ensure they match\n",
        "\n",
        "# ‚úÖ Define the model\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=10000, output_dim=32, input_length=200),\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# ‚úÖ Checkpoint callback\n",
        "checkpoint = ModelCheckpoint('model.h5.keras', monitor='val_loss', save_best_only=True)\n",
        "\n",
        "# ‚úÖ Train the model\n",
        "history = model.fit(X_train_padded, y_train,\n",
        "                    epochs=10, batch_size=32,\n",
        "                    validation_data=(X_test_padded, y_test),\n",
        "                    callbacks=[checkpoint])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwriYnK0rz4J"
      },
      "source": [
        "######################### Model Evaluation #########################\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ui4uje9Fr01Q",
        "outputId": "75b52713-75a2-408e-ddb1-c15883136d99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m3/3\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique predictions in y_pred: {1}\n",
            "Accuracy: 0.5, Precision: 0.5, Recall: 1.0, F1-score: 0.6666666666666666\n"
          ]
        }
      ],
      "source": [
        "# Ensure test data is properly tokenized and padded\n",
        "y_pred = model.predict(X_test_padded)  # ‚úÖ Use Padded Test Data\n",
        "y_pred = (y_pred > 0.5).astype(int)\n",
        "\n",
        "# Debugging: Check unique predictions\n",
        "print(\"Unique predictions in y_pred:\", set(y_pred.flatten()))\n",
        "\n",
        "# Performance Metrics\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "prec = precision_score(y_test, y_pred, zero_division=1)\n",
        "rec = recall_score(y_test, y_pred, zero_division=1)\n",
        "f1 = f1_score(y_test, y_pred, zero_division=1)\n",
        "model.save(\"sentiment_model.h5\")\n",
        "model.save('my_model.keras')\n",
        "\n",
        "print(f\"Accuracy: {acc}, Precision: {prec}, Recall: {rec}, F1-score: {f1}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFgGy-cFsGPC"
      },
      "source": [
        "Metrices\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3mwf3XCnsH4-",
        "outputId": "4dfda16a-d450-420a-dc2c-0029b9bbd74b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.5\n",
            "Precision: 0.5\n",
            "Recall: 1.0\n",
            "F1 Score: 0.6666666666666666\n"
          ]
        }
      ],
      "source": [
        "# Print performance metrics\n",
        "print('Accuracy:', acc)\n",
        "print('Precision:', prec)\n",
        "print('Recall:', rec)\n",
        "print('F1 Score:', f1)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "mount_file_id": "1z_UtInHzN16Iajj6SSIjbfFUbF1_3Z9V",
      "authorship_tag": "ABX9TyOzfxBJ4YpB/Tc/mwvvx3Fg",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}